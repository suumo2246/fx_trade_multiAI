# ğŸ–¥ï¸ GPUç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¬ã‚¤ãƒ‰

## ğŸ“‹ å‰ææ¡ä»¶

### ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢è¦ä»¶
- **NVIDIA GPU**: GTX 1060 6GBä»¥ä¸Šæ¨å¥¨ï¼ˆFXå–å¼•ã§ã¯ RTX 3060 12GBä»¥ä¸ŠãŒç†æƒ³ï¼‰
- **ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒ¢ãƒª**: 16GBä»¥ä¸Šæ¨å¥¨
- **ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸**: SSD 100GBä»¥ä¸Šã®ç©ºãå®¹é‡

### ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢è¦ä»¶
- **Windows 10/11** ã¾ãŸã¯ **Linux (Ubuntu 20.04+)**
- **Python 3.8-3.11** (3.12ã¯ä¸€éƒ¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§éå¯¾å¿œ)

## ğŸ”§ ã‚¹ãƒ†ãƒƒãƒ—1: NVIDIAç’°å¢ƒã®æº–å‚™

### 1.1 NVIDIA Driverã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
```bash
# ç¾åœ¨ã®ãƒ‰ãƒ©ã‚¤ãƒãƒ¼ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèª
nvidia-smi
```

æœ€æ–°ç‰ˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰: [NVIDIA Driver Downloads](https://www.nvidia.com/Download/index.aspx)

### 1.2 CUDA Toolkit ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
CUDA 11.8 ã¾ãŸã¯ 12.x ã‚’æ¨å¥¨

**Windows:**
1. [CUDA Toolkit Archive](https://developer.nvidia.com/cuda-toolkit-archive) ã‹ã‚‰ CUDA 11.8 ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
2. ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å¾Œã€ç’°å¢ƒå¤‰æ•°ã‚’ç¢ºèª:
```cmd
nvcc --version
```

**Linux:**
```bash
# CUDA 11.8 ã®å ´åˆ
wget https://developer.download.nvidia.com/compute/cuda/11.8.0/local_installers/cuda_11.8.0_520.61.05_linux.run
sudo sh cuda_11.8.0_520.61.05_linux.run
```

### 1.3 cuDNN ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
1. [cuDNN Archive](https://developer.nvidia.com/cudnn-archive) ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
2. CUDA ãƒ•ã‚©ãƒ«ãƒ€ã«å±•é–‹

## ğŸš€ ã‚¹ãƒ†ãƒƒãƒ—2: GPUå¯¾å¿œPythonãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

### 2.1 åŸºæœ¬ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
```bash
# ä»®æƒ³ç’°å¢ƒä½œæˆãƒ»æœ‰åŠ¹åŒ–
python -m venv venv_gpu
# Windows
venv_gpu\Scripts\activate
# Linux
source venv_gpu/bin/activate

# pip ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰
pip install --upgrade pip setuptools wheel
```

### 2.2 PyTorch (CUDAå¯¾å¿œç‰ˆ) ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
```bash
# CUDA 11.8 ã®å ´åˆ
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# CUDA 12.1 ã®å ´åˆ
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# CPUç‰ˆï¼ˆæ¯”è¼ƒç”¨ï¼‰
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
```

### 2.3 åŸºæœ¬GPUå¯¾å¿œãƒ©ã‚¤ãƒ–ãƒ©ãƒª
```bash
# æ©Ÿæ¢°å­¦ç¿’ï¼ˆGPUå¯¾å¿œï¼‰
pip install xgboost lightgbm catboost

# GPUç›£è¦–ãƒ„ãƒ¼ãƒ«
pip install nvidia-ml-py3 gpustat pynvml

# ãã®ä»–GPUå¯¾å¿œãƒ©ã‚¤ãƒ–ãƒ©ãƒª
pip install cupy-cuda11x  # CUDA 11.xç”¨
# pip install cupy-cuda12x  # CUDA 12.xç”¨
```

### 2.4 FXç‰¹åŒ–ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
```bash
# ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«åˆ†æ
pip install pandas numpy matplotlib seaborn
pip install ta finta mplfinance>=0.12.7a0
pip install backtesting

# æ™‚ç³»åˆ—åˆ†æï¼ˆGPUå¯¾å¿œï¼‰
pip install darts neuralprophet

# å¼·åŒ–å­¦ç¿’
pip install stable-baselines3 ray[rllib]
```

### 2.5 RAPIDS (ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãƒ»Linuxæ¨å¥¨)
```bash
# Condaç’°å¢ƒã§ã®ã¿æ¨å¥¨
conda install -c rapidsai -c nvidia -c conda-forge \
    cudf=23.10 cuml=23.10 cugraph=23.10 cuspatial=23.10 \
    python=3.10 cudatoolkit=11.8
```

## âœ… ã‚¹ãƒ†ãƒƒãƒ—3: GPUç’°å¢ƒã®ç¢ºèª

### 3.1 GPUèªè­˜ãƒ†ã‚¹ãƒˆ
```python
# gpu_check.py
import torch
import nvidia_ml_py3 as nvml

def check_gpu_environment():
    print("ğŸ” GPUç’°å¢ƒãƒã‚§ãƒƒã‚¯")
    print("=" * 50)
    
    # CUDAåˆ©ç”¨å¯èƒ½æ€§
    print(f"CUDAåˆ©ç”¨å¯èƒ½: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        print(f"CUDA ãƒãƒ¼ã‚¸ãƒ§ãƒ³: {torch.version.cuda}")
        print(f"GPU ãƒ‡ãƒã‚¤ã‚¹æ•°: {torch.cuda.device_count()}")
        
        for i in range(torch.cuda.device_count()):
            gpu_name = torch.cuda.get_device_name(i)
            gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
            print(f"GPU {i}: {gpu_name} ({gpu_memory:.1f} GB)")
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨çŠ¶æ³
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            allocated = torch.cuda.memory_allocated() / 1024**3
            reserved = torch.cuda.memory_reserved() / 1024**3
            print(f"GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {allocated:.2f} GB / {reserved:.2f} GB")
    
    # NVIDIA-MLç¢ºèª
    try:
        nvml.nvmlInit()
        driver_version = nvml.nvmlSystemGetDriverVersion()
        print(f"NVIDIA Driver: {driver_version}")
        
        device_count = nvml.nvmlDeviceGetCount()
        for i in range(device_count):
            handle = nvml.nvmlDeviceGetHandleByIndex(i)
            info = nvml.nvmlDeviceGetMemoryInfo(handle)
            name = nvml.nvmlDeviceGetName(handle)
            print(f"GPU {i}: {name.decode()} - Memory: {info.used/1024**3:.2f}/{info.total/1024**3:.2f} GB")
    
    except Exception as e:
        print(f"NVIDIA-ML ã‚¨ãƒ©ãƒ¼: {e}")

def test_gpu_computation():
    print("\nğŸ§® GPUè¨ˆç®—ãƒ†ã‚¹ãƒˆ")
    print("=" * 50)
    
    if torch.cuda.is_available():
        device = torch.device('cuda')
        
        # ç°¡å˜ãªè¡Œåˆ—æ¼”ç®—
        import time
        size = 5000
        
        # CPU
        start = time.time()
        a_cpu = torch.randn(size, size)
        b_cpu = torch.randn(size, size)
        c_cpu = torch.matmul(a_cpu, b_cpu)
        cpu_time = time.time() - start
        
        # GPU
        start = time.time()
        a_gpu = torch.randn(size, size, device=device)
        b_gpu = torch.randn(size, size, device=device)
        c_gpu = torch.matmul(a_gpu, b_gpu)
        torch.cuda.synchronize()
        gpu_time = time.time() - start
        
        print(f"CPUè¨ˆç®—æ™‚é–“: {cpu_time:.3f}ç§’")
        print(f"GPUè¨ˆç®—æ™‚é–“: {gpu_time:.3f}ç§’")
        print(f"GPUåŠ é€Ÿæ¯”: {cpu_time/gpu_time:.1f}å€")
    else:
        print("âŒ CUDA ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")

if __name__ == "__main__":
    check_gpu_environment()
    test_gpu_computation()
```

### 3.2 æ©Ÿæ¢°å­¦ç¿’ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®GPUãƒ†ã‚¹ãƒˆ
```python
# gpu_ml_test.py
def test_xgboost_gpu():
    try:
        import xgboost as xgb
        print("âœ… XGBoost GPUå¯¾å¿œãƒ†ã‚¹ãƒˆ")
        
        # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
        from sklearn.datasets import make_classification
        X, y = make_classification(n_samples=10000, n_features=20, random_state=42)
        
        # GPUç‰ˆ
        model = xgb.XGBClassifier(
            tree_method='gpu_hist',
            gpu_id=0,
            n_estimators=100
        )
        model.fit(X, y)
        print("âœ… XGBoost GPUå­¦ç¿’æˆåŠŸ")
    except Exception as e:
        print(f"âŒ XGBoost GPU ã‚¨ãƒ©ãƒ¼: {e}")

def test_lightgbm_gpu():
    try:
        import lightgbm as lgb
        print("âœ… LightGBM GPUå¯¾å¿œãƒ†ã‚¹ãƒˆ")
        
        from sklearn.datasets import make_classification
        X, y = make_classification(n_samples=10000, n_features=20, random_state=42)
        
        # GPUç‰ˆ
        model = lgb.LGBMClassifier(
            device='gpu',
            gpu_platform_id=0,
            gpu_device_id=0,
            n_estimators=100
        )
        model.fit(X, y)
        print("âœ… LightGBM GPUå­¦ç¿’æˆåŠŸ")
    except Exception as e:
        print(f"âŒ LightGBM GPU ã‚¨ãƒ©ãƒ¼: {e}")

if __name__ == "__main__":
    test_xgboost_gpu()
    test_lightgbm_gpu()
```

## âš™ï¸ FXå–å¼•ã§ã® GPUæ´»ç”¨ä¾‹

### 4.1 ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç‰¹å¾´é‡è¨ˆç®—
```python
import cupy as cp  # GPUç‰ˆNumPy
import cudf as cdf  # GPUç‰ˆPandas

# GPU ã§ã®é«˜é€Ÿãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™è¨ˆç®—
def gpu_calculate_ema(prices, period=9):
    prices_gpu = cp.asarray(prices)
    alpha = 2.0 / (period + 1)
    ema = cp.zeros_like(prices_gpu)
    ema[0] = prices_gpu[0]
    
    for i in range(1, len(prices_gpu)):
        ema[i] = alpha * prices_gpu[i] + (1 - alpha) * ema[i-1]
    
    return cp.asnumpy(ema)
```

### 4.2 å¤§è¦æ¨¡ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆ
```python
# è¤‡æ•°ã‚·ãƒŠãƒªã‚ªä¸¦åˆ—å®Ÿè¡Œ
import ray

@ray.remote(num_gpus=0.25)  # GPUåˆ†å‰²ä½¿ç”¨
def gpu_backtest_scenario(scenario_params, data):
    # ã‚·ãƒŠãƒªã‚ªåˆ¥ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    return run_backtest_gpu(scenario_params, data)

# ä¸¦åˆ—å®Ÿè¡Œ
futures = []
for scenario in scenario_list:
    future = gpu_backtest_scenario.remote(scenario, market_data)
    futures.append(future)

results = ray.get(futures)
```

## ğŸš¨ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### ã‚ˆãã‚ã‚‹ã‚¨ãƒ©ãƒ¼

**1. CUDA ãƒ¡ãƒ¢ãƒªä¸è¶³**
```python
# ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
torch.cuda.empty_cache()

# ãƒãƒƒãƒã‚µã‚¤ã‚ºèª¿æ•´
batch_size = 1024 if torch.cuda.get_device_properties(0).total_memory > 8e9 else 512
```

**2. CUDA ãƒãƒ¼ã‚¸ãƒ§ãƒ³ä¸æ•´åˆ**
```bash
# ç¾åœ¨ã®ç’°å¢ƒç¢ºèª
python -c "import torch; print(torch.version.cuda)"
nvcc --version

# PyTorchå†ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip uninstall torch torchvision torchaudio
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

**3. ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®GPUèªè­˜å¤±æ•—**
```bash
# ç’°å¢ƒå¤‰æ•°è¨­å®šï¼ˆWindowsï¼‰
set CUDA_PATH=C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.8

# ç’°å¢ƒå¤‰æ•°è¨­å®šï¼ˆLinuxï¼‰
export CUDA_HOME=/usr/local/cuda-11.8
export PATH=$CUDA_HOME/bin:$PATH
export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH
```

## ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

### GPUä½¿ç”¨é‡ç›£è¦–
```bash
# ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–
watch -n 1 nvidia-smi

# Python ã‹ã‚‰ç›£è¦–
gpustat -i 1
```

### ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æœ€é©åŒ–
```python
# ãƒ¢ãƒ‡ãƒ«è¨“ç·´æ™‚ã®è¨­å®š
import os
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'

# æ··åˆç²¾åº¦è¨“ç·´ï¼ˆãƒ¡ãƒ¢ãƒªå‰Šæ¸›ï¼‰
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()
with autocast():
    output = model(input)
    loss = criterion(output, target)
```

ã“ã‚Œã§GPUç’°å¢ƒãŒæ§‹ç¯‰ã§ããŸã‚‰ã€FXè‡ªå‹•ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚·ã‚¹ãƒ†ãƒ ã§å¤§å¹…ãªé«˜é€ŸåŒ–ã‚’å®Ÿç¾ã§ãã¾ã™ï¼